{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Εργαστηριακή Άσκηση 3 : Βαθία μάθηση\n"
      ],
      "metadata": {
        "id": "sgHGEkKauwBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Σε αυτή την εργαστηριακή άσκηση θα επικεντρωθούμε στην μοντελοποίηση ενός νευρωνικού δικτύου παραγωγής λεκτικών περιγραφών από εικόνες (Image Captioning). Το dataset που θα χρησιμοποιήσουμε είναι το \"flickr30k-images-ecemod\", μια παραλλαγή του flick30k για το μάθημά μας. Στo flickr30k-images-ecemod κάθε εικόνα έχει 5 captions. Συγκεκριμένα, τα δεδομένα του flickr30k-images-ecemod είναι τα εξής : \n",
        "* ένας φάκελος \"image_dir\" με 31.783 εικόνες από το Flickr\n",
        "* ένα αρχείο \"train_captions.csv\" με 148.915 captions για τις εικόνες του \"image_dir\"\n",
        "* ένα αρχείο \"test_images.csv\" με 2.000 ονόματα εικόνων από το \"imag_dir\" που δεν έχουν captions εκπαίδευσης"
      ],
      "metadata": {
        "id": "7cnJ2d0muwBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################\n",
        "\"\"\" Import Libraries \"\"\"\n",
        "########################\n",
        "\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import collections\n",
        "import random\n",
        "import re\n",
        "from glob import glob\n",
        "import time\n",
        "import json\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "#from kaggle_datasets import KaggleDatasets\n",
        "#import efficientnet.tfkeras as efn\n",
        "\n",
        "######################################################\n",
        "\"\"\" Useless Functions to check stuff in text files \"\"\"\n",
        "######################################################\n",
        "\n",
        "#def to show image\n",
        "def PlotarismaIMG(eikona, titlos):\n",
        "    plt.imshow(eikona) \n",
        "    plt.title(titlos)\n",
        "    plt.show()\n",
        "    return None\n",
        "\n",
        "#def to show image in grayscale\n",
        "def grayIMG(eikona, titlos):\n",
        "    plt.imshow(eikona, cmap = 'gray') \n",
        "    plt.title(titlos)\n",
        "    plt.show()\n",
        "    return None\n",
        "\n",
        "#def to show image in grayscale\n",
        "def grayRevIMG(eikona, titlos):\n",
        "    plt.imshow(eikona, cmap = 'gray_r') \n",
        "    plt.title(titlos)\n",
        "    plt.show()\n",
        "    return None\n",
        "\n",
        "def axristi_diaxorisi():\n",
        "    print(\"o----------------------o\",\n",
        "          \"o----------------------o\",\n",
        "          \"o----------------------o\",\n",
        "          \"o----------------------o\",\n",
        "          \"o----------------------o\",\"\\n\")\n",
        "    return None"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T17:58:24.996208Z",
          "iopub.execute_input": "2022-03-13T17:58:24.996874Z",
          "iopub.status.idle": "2022-03-13T17:58:29.780700Z",
          "shell.execute_reply.started": "2022-03-13T17:58:24.996834Z",
          "shell.execute_reply": "2022-03-13T17:58:29.779960Z"
        },
        "trusted": true,
        "id": "zRgXvp8QuwBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Στο επόμενο κελί κατεβάζουμε τα απαιτούμενα datasets για την μοντελοποίηση του δικτύου μας, δηλαδή τις εικόνες του dataset (31.782 + 1 read_me.txt) στον φάκελο _image_dir_, τα _train_captions.csv_ που περιέχουν paths για τις εικόνες μέσα στο φάκελο _image_dir_ με τα αντίστοιχα captions τους, και τα _test_images.csv_ που περιέχουν paths για τις εικόνες μέσα στο φάκελο _image_dir_ , οι οποίες δεν έχουν caption"
      ],
      "metadata": {
        "id": "U9W-Po7wuwBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download image files\n",
        "image_zip = tf.keras.utils.get_file('flickr30k-images-ecemod.zip',\n",
        "                                    cache_subdir = os.path.abspath('.'),\n",
        "                                    origin='https://spartacus.1337.cx/flickr-mod/flickr30k-images-ecemod.zip',\n",
        "                                    extract=True)\n",
        "\n",
        "image_dir_folder = os.path.dirname(image_zip)+\"/image_dir\"\n",
        "os.remove(image_zip)\n",
        "\n",
        "# Download train captions file\n",
        "# every train img has 5 captions\n",
        "train_captions_file = tf.keras.utils.get_file('train_captions.csv',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='https://spartacus.1337.cx/flickr-mod/train_captions.csv',\n",
        "                                           extract=False)\n",
        "\n",
        "# Download test files list\n",
        "test_images_file = tf.keras.utils.get_file('test_images.csv',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='https://spartacus.1337.cx/flickr-mod/test_images.csv',\n",
        "                                           extract=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T17:58:29.782362Z",
          "iopub.execute_input": "2022-03-13T17:58:29.782627Z",
          "iopub.status.idle": "2022-03-13T18:03:52.674913Z",
          "shell.execute_reply.started": "2022-03-13T17:58:29.782593Z",
          "shell.execute_reply": "2022-03-13T18:03:52.674215Z"
        },
        "trusted": true,
        "id": "hNjWcbN5uwBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# put all images paths in one list\n",
        "image_dir_files = os.listdir(image_dir_folder)\n",
        "Images_Paths = []\n",
        "for i in range(0,len(image_dir_files)):\n",
        "    if(image_dir_files[i][-3:]==\"jpg\"):\n",
        "        Images_Paths.append(\"/kaggle/working/image_dir/\"+image_dir_files[i])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:03:52.676563Z",
          "iopub.execute_input": "2022-03-13T18:03:52.676816Z",
          "iopub.status.idle": "2022-03-13T18:03:52.714446Z",
          "shell.execute_reply.started": "2022-03-13T18:03:52.676780Z",
          "shell.execute_reply": "2022-03-13T18:03:52.713792Z"
        },
        "trusted": true,
        "id": "D4njLDUauwBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ελάττωση πλήθους δεδομένων\n",
        "\n",
        "Στα επόμενα κελιά δημιουργούνται δύο συναρτήσεις, οι οποίες θα βοηθήσουν στην επιτάχυνση της διαδικασίας της εκπαίδευσης, ενώ παράλληλα κρίνονται αναγκαίες δεδομένου ότι εργαζόμαστε στο kaggle και η παρεχόμενη μνήμη RAM είναι περιορισμένη. Αναλυτικά, οι συναρτήσεις λειτουργούν ως εξής : \n",
        "* **group_captions** δέχεται ως είσοδο το train.csv αρχείο και ομαδοποιεί τα captions κάθε εικόνας, επιστρέφοντας στην έξοδο ένα dictionary με keys τα paths για κάθε εικόνα και values τα 5 captions που αντιστοιχούν σε κάθε εικόνα\n",
        "* **group_imgs** δέχεται ως είσοδο τον φάκελο _image_dir_ και επιστρέφει ένα dictionary με keys τα paths για κάθε εικόνα και values τις εικόνες σε \"αναγνώσιμη\" μορφή με χρήση της _PIL.Image_"
      ],
      "metadata": {
        "id": "8lf0C8VnuwBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "\"\"\" Group Captions \"\"\"\n",
        "######################\n",
        "\n",
        "def group_captions(FILE):\n",
        "    \n",
        "    with open(FILE, \"r\") as f:\n",
        "        \n",
        "        lines = f.readlines()\n",
        "    \n",
        "        # Get the ids\n",
        "        ids = []\n",
        "        for line in lines:\n",
        "            ids.append(line.split(\".\")[0])\n",
        "    \n",
        "        # Get the captions\n",
        "        captions = []\n",
        "        for line in lines:\n",
        "            captions_helper = line.split(\"|\")[2]\n",
        "            captions_helper2 = captions_helper.rstrip(\"\\n\")\n",
        "            captions.append(captions_helper2)\n",
        "    \n",
        "        #checker = ids[0]\n",
        "        image_path_to_caption = collections.defaultdict(list)\n",
        "        for iD in range(0,len(ids)):\n",
        "            caption = \"<start> \"+captions[iD]+\" <end>\"\n",
        "            path_2_img = \"/kaggle/working/image_dir/\"+ids[iD]+\".jpg\"\n",
        "            image_path_to_caption[path_2_img].append(caption)\n",
        "    \n",
        "        #print(list(image_path_to_caption.keys())[0])\n",
        "        #print(list(image_path_to_caption.values())[0])\n",
        "    \n",
        "    return image_path_to_caption"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:03:52.716475Z",
          "iopub.execute_input": "2022-03-13T18:03:52.716718Z",
          "iopub.status.idle": "2022-03-13T18:03:52.723590Z",
          "shell.execute_reply.started": "2022-03-13T18:03:52.716684Z",
          "shell.execute_reply": "2022-03-13T18:03:52.722926Z"
        },
        "trusted": true,
        "id": "DooozopQuwBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path_to_train_caption = group_captions(train_captions_file)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:03:52.724798Z",
          "iopub.execute_input": "2022-03-13T18:03:52.725447Z",
          "iopub.status.idle": "2022-03-13T18:03:53.215478Z",
          "shell.execute_reply.started": "2022-03-13T18:03:52.725395Z",
          "shell.execute_reply": "2022-03-13T18:03:53.214692Z"
        },
        "trusted": true,
        "id": "sB18uGjcuwBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "\"\"\" Group Images \"\"\"\n",
        "####################\n",
        "\n",
        "def group_imgs(folder_path):\n",
        "    \n",
        "    jpgs = []\n",
        "    jpg_names = os.listdir(folder_path)\n",
        "    image_id = [] \n",
        "    for jpg_name in jpg_names:\n",
        "        if jpg_name[-3:]== \"jpg\":\n",
        "            jpgs.append(Image.open(folder_path+\"/\"+jpg_name))\n",
        "            image_id.append(jpg_name.split(\".\")[0])\n",
        "    \n",
        "    image_path_to_jpg = collections.defaultdict(list)\n",
        "    for iD in range(0,len(image_id)):\n",
        "        path_2_img = \"/kaggle/working/image_dir/\"+image_id[iD]+\".jpg\"\n",
        "        image_path_to_jpg[path_2_img].append(jpgs[iD])\n",
        "    \n",
        "    #print(list(image_path_to_jpg.keys())[0])\n",
        "    #print(list(image_path_to_jpg.values())[0])\n",
        "    \n",
        "    return image_path_to_jpg"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:03:53.216641Z",
          "iopub.execute_input": "2022-03-13T18:03:53.216874Z",
          "iopub.status.idle": "2022-03-13T18:03:53.225247Z",
          "shell.execute_reply.started": "2022-03-13T18:03:53.216842Z",
          "shell.execute_reply": "2022-03-13T18:03:53.224289Z"
        },
        "trusted": true,
        "id": "1JHLpyZVuwBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir_folder = os.path.dirname(image_zip)+\"/image_dir\"\n",
        "image_path_to_jpg2 = group_imgs(image_dir_folder)\n",
        "#print(list(image_path_to_jpg2.keys())[:5])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:03:53.226813Z",
          "iopub.execute_input": "2022-03-13T18:03:53.227806Z",
          "iopub.status.idle": "2022-03-13T18:03:58.950108Z",
          "shell.execute_reply.started": "2022-03-13T18:03:53.227769Z",
          "shell.execute_reply": "2022-03-13T18:03:58.949407Z"
        },
        "trusted": true,
        "id": "3Rdtqm6XuwBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "\"\"\" Show Images with their captions \"\"\"\n",
        "#######################################\n",
        "\n",
        "def IMG_captions(File_name, path_2_jpg=image_path_to_jpg2, path_2_caption=image_path_to_train_caption):\n",
        "    path_2_image_dir = \"/kaggle/working/image_dir/\"\n",
        "    PlotarismaIMG(path_2_jpg[path_2_image_dir+File_name][0], \" \")\n",
        "    print(\"The captions of the image \"+File_name+\" are : \\n\")\n",
        "    print(path_2_caption[path_2_image_dir+File_name])\n",
        "    return None"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:03:58.951570Z",
          "iopub.execute_input": "2022-03-13T18:03:58.952048Z",
          "iopub.status.idle": "2022-03-13T18:03:58.957891Z",
          "shell.execute_reply.started": "2022-03-13T18:03:58.952011Z",
          "shell.execute_reply": "2022-03-13T18:03:58.957197Z"
        },
        "trusted": true,
        "id": "pX_JmFe8uwBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_captions(\"_124857180.jpg\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:03:58.959339Z",
          "iopub.execute_input": "2022-03-13T18:03:58.959895Z",
          "iopub.status.idle": "2022-03-13T18:03:59.200734Z",
          "shell.execute_reply.started": "2022-03-13T18:03:58.959860Z",
          "shell.execute_reply": "2022-03-13T18:03:59.200081Z"
        },
        "trusted": true,
        "id": "-J9a70gIuwBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
        "    ## choose the suitable preprocessing for CNN\n",
        "    #img = tf.keras.applications.inception_v3.preprocess_input(img) \n",
        "    img = tf.keras.applications.xception.preprocess_input(img)\n",
        "    return img, image_path"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:03:59.203606Z",
          "iopub.execute_input": "2022-03-13T18:03:59.204243Z",
          "iopub.status.idle": "2022-03-13T18:03:59.210413Z",
          "shell.execute_reply.started": "2022-03-13T18:03:59.204205Z",
          "shell.execute_reply": "2022-03-13T18:03:59.209442Z"
        },
        "trusted": true,
        "id": "QRaEgeFEuwBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img1,_ = load_image(\"/kaggle/working/image_dir/_1000070808.jpg\")\n",
        "PlotarismaIMG(img1, \" \")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:03:59.211724Z",
          "iopub.execute_input": "2022-03-13T18:03:59.212129Z",
          "iopub.status.idle": "2022-03-13T18:04:01.761932Z",
          "shell.execute_reply.started": "2022-03-13T18:03:59.212086Z",
          "shell.execute_reply": "2022-03-13T18:04:01.761249Z"
        },
        "trusted": true,
        "id": "Guyn0QoVuwBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Δημιουργία Keras μοντέλου αρχιτεκτονικής InceptionV3 - Xception\n",
        "\n",
        "Στο επόμενο κελί δημιουργούμε ένα _tf.keras_ μοντέλο, όπου το τελευταίο επίπεδο (layer) είναι το τελευταίο συνελκτικό επίπεδο στην InceptionV3 αρχιτεκτονική. Οι διαστάσεις του output_layer θα είναι 8x8x2048 για InceptionV3 ή 10x10x1280 για EfficientNetB1. Το τελευταίο συνελκτικό επίπεδο το χρησιμοποιούμε γιατί η λογική μάθησης του μοντέλου είναι βασισμένη στην attention. Ακολούθως γίνονται τα παρακάτω βήματα, \n",
        "* κάθε εικόνα \"περνάει\" από το δίκτυο και το διάνυσμα που προκύπτει αποθηκεύεται σε ένα dictionary (image_name --> feature_vector)\n",
        "* αυτό το dictionary αποθηκεύεται στον δίσκο"
      ],
      "metadata": {
        "id": "4lRmPIFCuwBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "\"\"\" Initialize InceptionV3 - Xception \"\"\"\n",
        "#########################################\n",
        "\n",
        "image_model = tf.keras.applications.Xception(include_top=False,weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:04:01.763290Z",
          "iopub.execute_input": "2022-03-13T18:04:01.764109Z",
          "iopub.status.idle": "2022-03-13T18:04:03.423211Z",
          "shell.execute_reply.started": "2022-03-13T18:04:01.764069Z",
          "shell.execute_reply": "2022-03-13T18:04:03.422449Z"
        },
        "trusted": true,
        "id": "dgLUu-DGuwBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the model that we've mad\n",
        "#print(image_model.summary())\n",
        "axristi_diaxorisi()\n",
        "print(\"Information of the CNN output\")\n",
        "print(image_model.layers[-1], image_model.layers[-1].output.shape)\n",
        "#print(image_model)\n",
        "axristi_diaxorisi()\n",
        "print(\"New_input \\n\",new_input)\n",
        "axristi_diaxorisi()\n",
        "print(\"Hidden_layer \\n\",hidden_layer)\n",
        "axristi_diaxorisi()\n",
        "print(\"Image_features_extract_model \\n\",image_features_extract_model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:04:03.425357Z",
          "iopub.execute_input": "2022-03-13T18:04:03.425853Z",
          "iopub.status.idle": "2022-03-13T18:04:03.438585Z",
          "shell.execute_reply.started": "2022-03-13T18:04:03.425814Z",
          "shell.execute_reply": "2022-03-13T18:04:03.437196Z"
        },
        "trusted": true,
        "id": "bsS7XBf3uwBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Λήψη Δείγματος Δεδομένων\n",
        "\n",
        "Στο επόμενο κελί επιλέγεται ένας συγκεκριμένος αριθμός εικόνων _Samples_ , ο οποίος θα αποτελέσει το σύνολο εκπαίδευσης, μιας και στο kaggle η παρεχόμενη μνήμη RAM είναι περιορισμένη. Πρέπει να σημειωθεί ότι αυτή η διαδικασία θα μειώσει την επιτυχία αποφάσης ακριβέστερου caption, όμως θα επιταχύνει τη διαδικασία. \n",
        "\n",
        "Ο img_name_train_vector περιλαμβάνει τα paths των εικόνων που θα χρησιμοποιήσουμε για την εκπαίδευση του μοντέλου.Το length του είναι 30.000 και προκύπτει ως το πλήθος των εικόνων που επιλέξαμε για την εκπαιδευση , δηλαδή 6.000 ,επί το πλήθος των περιγραφών για την κάθε εικόνα οι οποίες όπως αναφέραμε προηγουμένως είναι 5.Συνεπώς αν εκτυπώσουμε τις 5 πρώτες τιμές του img_name_train_vector θα δούμε ότι τυπώνουν το ίδιο path.\n",
        "\n",
        "Αντίστοιχα ο πίνακας captions_train έχει πάλι length 30.000 και περιλαμβάνει όλα τα captions για όλες τις εικόνες που θα χρησιμοποιήσουμε για την εκπαίδευση του μοντέλου.Αν χρησιμοποιήσουμε το indexing ενός συγκεκριμένου caption στο img_name_train_vector μπορούμε να δούμε σε ποια εικόνα αναφέρεται."
      ],
      "metadata": {
        "id": "0H5RVwqEuwBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################\n",
        "\"\"\" Get The Unique Images \"\"\"\n",
        "#############################\n",
        "\n",
        "def Get_Images_Captions_Vectors(img_path_dict, Samples):\n",
        "    \n",
        "    # Get unique images by following the TENSORFLOW TUTORIAL\n",
        "    Image_Paths = list(img_path_dict.keys())[:Samples]\n",
        "    captions_vector = []\n",
        "    img_name_vector = []\n",
        "\n",
        "    for image_path in Image_Paths:\n",
        "        Captions_InList = img_path_dict[image_path]\n",
        "        captions_vector.extend(Captions_InList)\n",
        "        img_name_vector.extend([image_path] * len(Captions_InList))\n",
        "\n",
        "    print(captions_vector[0])\n",
        "    PlotarismaIMG(Image.open(img_name_vector[0]),\" \")    \n",
        "    \n",
        "    #Encoded = sorted(set(img_name_vector))\n",
        "    \n",
        "    return img_name_vector, captions_vector"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:04:03.439941Z",
          "iopub.execute_input": "2022-03-13T18:04:03.440200Z",
          "iopub.status.idle": "2022-03-13T18:04:04.807595Z",
          "shell.execute_reply.started": "2022-03-13T18:04:03.440166Z",
          "shell.execute_reply": "2022-03-13T18:04:04.806578Z"
        },
        "trusted": true,
        "id": "K3mRT27NuwBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_name_train_vector, captions_train = Get_Images_Captions_Vectors(image_path_to_train_caption, 6000)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:04:04.809082Z",
          "iopub.execute_input": "2022-03-13T18:04:04.809436Z",
          "iopub.status.idle": "2022-03-13T18:04:05.043785Z",
          "shell.execute_reply.started": "2022-03-13T18:04:04.809382Z",
          "shell.execute_reply": "2022-03-13T18:04:05.040771Z"
        },
        "trusted": true,
        "id": "PlknycncuwBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Προεπεξεργασία και tokenization των captions\n",
        "\n",
        "Στο επομένο κελί γίνεται η προεπεξεργασία των captions με σπάσιμο σε λέξεις-tokens. Αυτά το tokens θα συντελέσουν στην κατασκευή ενός dictionary 5000 στοιχείων, όπου αντιστοιχεί κάθε λέξη στον index της στο vocabulary. Τέλος δημιουργούνται τα word-to-index και index-to-word mappings για να οπτικοποιηθούν τα αποτελέσματα.\n",
        "\n",
        "Αρχικά κρατάω τα captions με μήκος λέξεων στο range(22,46) εφόσον τα πολύ μικρά ή τα πολύ μεγάλα captions δεν μας δίνουν ωφέλιμη πληροφορία στην εκπαίδευση.\n",
        "\n",
        "Επιπλέον αυξάνω το μέγεθος του vocabulary στις 10000 λέξεις.\n",
        "\n",
        "Τέλος, στην standardize προσθέτω ένα ακόμα φίλτρο lemmatization και επισης αφαιρώ από τα captions τις λέξεις που έχουν μήκος χαρακτήρων μικρότερο του 3."
      ],
      "metadata": {
        "id": "zbdEsE7BuwBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "\"\"\" Preprocess and tokenize the captions \"\"\"\n",
        "############################################\n",
        "\n",
        "captions_train_optim=[]\n",
        "img_name_train_vector_optim=[]\n",
        "id_2_check = img_name_train_vector[0]\n",
        "is_not_0 = []\n",
        "for idx, caption in enumerate(captions_train):\n",
        "    #id_2_caption[idx] = img_name_train_vector[idx]\n",
        "    words=caption.split(' ')\n",
        "    #το μήκος της λέξης προκύπτει ως len(words)-4 λέξεις οι οποίες είναι το '<start>','<end>','.' και ένα κενό\n",
        "    if(len(words)<=50 and len(words)>=18):\n",
        "        captions_train_optim.append(caption)   \n",
        "        img_name_train_vector_optim.append(img_name_train_vector[idx])\n",
        "        if(id_2_check==img_name_train_vector[idx]):\n",
        "            is_not_0.append(False)\n",
        "    \n",
        "    if(len(words)>50 or len(words)<18):\n",
        "        is_not_0.append(True)\n",
        "    \n",
        "    if(((idx+1) % 5) == 0):\n",
        "        if(all(is_not_0)):\n",
        "            is_not_0 = []\n",
        "            captions_train_optim.append(caption)   \n",
        "            img_name_train_vector_optim.append(img_name_train_vector[idx])\n",
        "            if((idx+1)==len(captions_train)):\n",
        "                id_2_check=img_name_train_vector[idx]\n",
        "        else:\n",
        "            is_not_0 = []\n",
        "            if((idx+1)==len(captions_train)):\n",
        "                id_2_check=img_name_train_vector[idx]\n",
        "        \n",
        "print(len(captions_train))\n",
        "print(len(captions_train_optim))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:04:05.045010Z",
          "iopub.execute_input": "2022-03-13T18:04:05.045705Z",
          "iopub.status.idle": "2022-03-13T18:04:05.119718Z",
          "shell.execute_reply.started": "2022-03-13T18:04:05.045668Z",
          "shell.execute_reply": "2022-03-13T18:04:05.118968Z"
        },
        "trusted": true,
        "id": "R4N9bfkCuwBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Lemmatization of a caption \"\"\"\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def Lem(caption):\n",
        "    word_list=caption.split(' ')\n",
        "    lem_output=' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "    return lem_output"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:04:05.120988Z",
          "iopub.execute_input": "2022-03-13T18:04:05.121379Z",
          "iopub.status.idle": "2022-03-13T18:04:05.710432Z",
          "shell.execute_reply.started": "2022-03-13T18:04:05.121343Z",
          "shell.execute_reply": "2022-03-13T18:04:05.709744Z"
        },
        "trusted": true,
        "id": "7TVi7aCSuwBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Keep only the words with lentgh<=3\"\"\"\n",
        "captions_new=[]\n",
        "for caption in captions_train_optim:\n",
        "    lista=[]\n",
        "    word_list=caption.split(' ')\n",
        "    for idx in range(2,len(word_list)-2):\n",
        "        word=word_list[idx]\n",
        "        if len(word)>=3:\n",
        "            lista.append(word)\n",
        "    sentence=' '.join([w for w in lista])\n",
        "    sentences='<start>  ' + sentence + ' <end>'\n",
        "    captions_new.append(sentences)\n",
        "        \n",
        "            \n",
        "            "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:04:05.711675Z",
          "iopub.execute_input": "2022-03-13T18:04:05.711997Z",
          "iopub.status.idle": "2022-03-13T18:04:05.841566Z",
          "shell.execute_reply.started": "2022-03-13T18:04:05.711960Z",
          "shell.execute_reply": "2022-03-13T18:04:05.840755Z"
        },
        "trusted": true,
        "id": "My_TRrRRuwBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(captions_new[:5])\n",
        "print(captions_train_optim[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:04:05.842999Z",
          "iopub.execute_input": "2022-03-13T18:04:05.843267Z",
          "iopub.status.idle": "2022-03-13T18:04:05.849462Z",
          "shell.execute_reply.started": "2022-03-13T18:04:05.843232Z",
          "shell.execute_reply": "2022-03-13T18:04:05.848555Z"
        },
        "trusted": true,
        "id": "T62LaF0CuwBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Lemmatization in the whole dataset captions_train_optim \"\"\"\n",
        "captions_train_lem=captions_new\n",
        "for caption in captions_train_optim:\n",
        "    captions_train_lemi.append(Lem(caption))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:05:29.856312Z",
          "iopub.execute_input": "2022-03-13T18:05:29.856951Z",
          "iopub.status.idle": "2022-03-13T18:05:29.860905Z",
          "shell.execute_reply.started": "2022-03-13T18:05:29.856913Z",
          "shell.execute_reply": "2022-03-13T18:05:29.859699Z"
        },
        "trusted": true,
        "id": "DJncFquOuwBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "caption_dataset = tf.data.Dataset.from_tensor_slices(captions_train_lem)\n",
        "\n",
        "# We will override the default standardization of TextVectorization to preserve\n",
        "# \"<>\" characters, so we preserve the tokens for the <start> and <end>.\n",
        "def standardize(inputs):\n",
        "    inputs = tf.strings.lower(inputs)\n",
        "    return tf.strings.regex_replace(inputs,r\"!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~\", \"\")\n",
        "\n",
        "# Max word count for a caption.\n",
        "max_length =45\n",
        "\n",
        "# Use the top 5000 words for a vocabulary.\n",
        "vocabulary_size =10000\n",
        "tokenizer = tf.keras.layers.TextVectorization(max_tokens=vocabulary_size,standardize=standardize,output_sequence_length=max_length)\n",
        "\n",
        "# Learn the vocabulary from the caption data.\n",
        "tokenizer.adapt(caption_dataset)\n",
        "\n",
        "# Create the tokenized vectors\n",
        "cap_vector = caption_dataset.map(lambda x: tokenizer(x))\n",
        "\n",
        "# Create mappings for words to indices and indicies to words.\n",
        "word_to_index = tf.keras.layers.StringLookup(mask_token=\"\",vocabulary=tokenizer.get_vocabulary())\n",
        "index_to_word = tf.keras.layers.StringLookup(mask_token=\"\",vocabulary=tokenizer.get_vocabulary(),invert=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:05:33.275421Z",
          "iopub.execute_input": "2022-03-13T18:05:33.275954Z",
          "iopub.status.idle": "2022-03-13T18:05:52.549244Z",
          "shell.execute_reply.started": "2022-03-13T18:05:33.275916Z",
          "shell.execute_reply": "2022-03-13T18:05:52.548541Z"
        },
        "trusted": true,
        "id": "bfNBOTokuwBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Caching τα διανύσματα από το InceptionV3\n",
        "\n",
        "Στο επόμενο κελί τα διανύσματα - χαρακτηριστικά που προέκυψαν από το μοντέλο keras αρχιτεκτονικής InceptionV3 αποθηκεύονται προσωρινά στην Cache."
      ],
      "metadata": {
        "id": "nj-h-HD9uwBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################################\n",
        "\"\"\" Caching the features extracted from InceptionV3 - Xception \"\"\"\n",
        "##################################################################\n",
        "\n",
        "encode_train = sorted(set(img_name_train_vector_optim))\n",
        "\n",
        "feature_dict = {}\n",
        "\n",
        "# Feel free to change batch_size according to your system configuration\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
        "\n",
        "for img, path in image_dataset:\n",
        "    batch_features = image_features_extract_model(img)\n",
        "    batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "    for bf, p in zip(batch_features, path):\n",
        "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "        np.save(path_of_feature, bf.numpy())\n",
        "        feature_dict[path_of_feature] =  bf.numpy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:05:56.076869Z",
          "iopub.execute_input": "2022-03-13T18:05:56.077323Z",
          "iopub.status.idle": "2022-03-13T18:07:18.139684Z",
          "shell.execute_reply.started": "2022-03-13T18:05:56.077286Z",
          "shell.execute_reply": "2022-03-13T18:07:18.138364Z"
        },
        "trusted": true,
        "id": "aCHMX01WuwBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Διαχωρισμός Δεδομένων σε train, validation και test set\n",
        "\n",
        "Στο παρακάτω κελί υλοποιείται ο διαχωρισμός σε train και dev set με καθοδήγηση από το tutorial του TensorFlow [\"Image captioning with visual attention\"](https://www.tensorflow.org/tutorials/text/image_captioning) ."
      ],
      "metadata": {
        "id": "n6uY_lOMuwBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "\"\"\" Split the data into train and dev \"\"\"\n",
        "#########################################\n",
        "\n",
        "img_to_cap_vector = collections.defaultdict(list)\n",
        "for img, cap in zip(img_name_train_vector, cap_vector):\n",
        "    img_to_cap_vector[img].append(cap)\n",
        "\n",
        "# Create training and validation sets using an 80-20 split randomly.\n",
        "img_keys = list(img_to_cap_vector.keys())\n",
        "#random.shuffle(img_keys)\n",
        "\n",
        "slice_index = int(len(img_keys)*0.8)\n",
        "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
        "\n",
        "img_name_train = []\n",
        "cap_train = []\n",
        "for imgt in img_name_train_keys:\n",
        "    capt_len = len(img_to_cap_vector[imgt])\n",
        "    img_name_train.extend([imgt] * capt_len)\n",
        "    cap_train.extend(img_to_cap_vector[imgt])\n",
        "\n",
        "img_name_val = []\n",
        "cap_val = []\n",
        "for imgv in img_name_val_keys:\n",
        "    capv_len = len(img_to_cap_vector[imgv])\n",
        "    img_name_val.extend([imgv] * capv_len)\n",
        "    cap_val.extend(img_to_cap_vector[imgv])\n",
        "    \n",
        "#Normalization in batch_size\n",
        "\n",
        "img_name_train = img_name_train[:len(img_name_train)-(len(img_name_train) % 100)]\n",
        "cap_train = cap_train[:len(cap_train)-(len(cap_train) % 100)]\n",
        "img_name_val = img_name_val[:len(img_name_val)-(len(img_name_val) % 100)]\n",
        "cap_val = cap_val[:len(cap_val)-(len(cap_val) % 100)]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:09:41.052810Z",
          "iopub.execute_input": "2022-03-13T18:09:41.053084Z",
          "iopub.status.idle": "2022-03-13T18:09:47.007333Z",
          "shell.execute_reply.started": "2022-03-13T18:09:41.053056Z",
          "shell.execute_reply": "2022-03-13T18:09:47.006625Z"
        },
        "trusted": true,
        "id": "LC_HnaTFuwBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(img_name_val))\n",
        "print(cap_train[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:09:47.008925Z",
          "iopub.execute_input": "2022-03-13T18:09:47.009153Z",
          "iopub.status.idle": "2022-03-13T18:09:47.014280Z",
          "shell.execute_reply.started": "2022-03-13T18:09:47.009120Z",
          "shell.execute_reply": "2022-03-13T18:09:47.013599Z"
        },
        "trusted": true,
        "id": "1LUdjXubuwBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Δημιουργία tf.data dataset για εκπαίδευση"
      ],
      "metadata": {
        "id": "a4cuGcsFuwBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "\"\"\" Create a tf.data dataset for training \"\"\"\n",
        "#############################################\n",
        "\n",
        "# Feel free to change these parameters according to your system's configuration\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 100\n",
        "units = 512\n",
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 100\n",
        "\n",
        "# Load the numpy files\n",
        "def map_func(img_name, cap):\n",
        "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "    return img_tensor, cap\n",
        "\n",
        "###------------------- train - dataset ------------------------###\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "train_dataset = train_dataset.map(lambda item1, item2: tf.numpy_function(map_func,[item1,item2],[tf.float32,tf.int64]),\n",
        "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Shuffle and batch\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "###------------------- val - dataset ------------------------###\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((img_name_val, cap_val))\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "val_dataset = val_dataset.map(lambda item1, item2: tf.numpy_function(map_func,[item1,item2],[tf.float32,tf.int64]),\n",
        "                              num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Shuffle and batch\n",
        "val_dataset = val_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:09:47.015613Z",
          "iopub.execute_input": "2022-03-13T18:09:47.016066Z",
          "iopub.status.idle": "2022-03-13T18:09:47.816520Z",
          "shell.execute_reply.started": "2022-03-13T18:09:47.016030Z",
          "shell.execute_reply": "2022-03-13T18:09:47.815790Z"
        },
        "trusted": true,
        "id": "ku3AVPdouwBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "cAxNThdquwBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Embeddings through gensim\"\"\"\n",
        "#!pip install -U gensim\n",
        "import pickle\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "a=list(gensim.downloader.info()['models'])\n",
        "print(a)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:09:47.820064Z",
          "iopub.execute_input": "2022-03-13T18:09:47.820465Z",
          "iopub.status.idle": "2022-03-13T18:09:48.139880Z",
          "shell.execute_reply.started": "2022-03-13T18:09:47.820427Z",
          "shell.execute_reply": "2022-03-13T18:09:48.139137Z"
        },
        "trusted": true,
        "id": "K8uVpuGsuwBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################################\n",
        "\"\"\" load the glove_wiki_gigaword models and dump them using the pickle method \"\"\"\n",
        "#################################################################################\n",
        "\n",
        "#model_50=gensim.downloader.load('glove-wiki-gigaword-50')\n",
        "#pickle.dump(model_50, open(\"glove_wiki_50\", 'wb'))\n",
        "\n",
        "model_100=gensim.downloader.load('glove-wiki-gigaword-100')\n",
        "pickle.dump(model_100, open(\"glove_wiki_100\", 'wb'))\n",
        "\n",
        "#model_200=gensim.downloader.load('glove-wiki-gigaword-200')\n",
        "#pickle.dump(model_200, open(\"glove_wiki_200\", 'wb'))\n",
        "\n",
        "#model_300=gensim.downloader.load('glove-wiki-gigaword-300')\n",
        "#pickle.dump(model_300, open(\"glove_wiki_300\", 'wb'))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:09:49.587655Z",
          "iopub.execute_input": "2022-03-13T18:09:49.588308Z",
          "iopub.status.idle": "2022-03-13T18:10:45.219524Z",
          "shell.execute_reply.started": "2022-03-13T18:09:49.588269Z",
          "shell.execute_reply": "2022-03-13T18:10:45.218761Z"
        },
        "trusted": true,
        "id": "6aAOZ8SSuwBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Χρησιμοποιούμε το μοντέλο με τα 100 embeddings καθώς τα ανώτερα μοντέλα των 200 και 300 εξαντλούν την μνήμη RAM του Kaggle."
      ],
      "metadata": {
        "id": "aEWXvfK-uwBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_100 = pickle.load(open('glove_wiki_100', 'rb'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:10:45.221159Z",
          "iopub.execute_input": "2022-03-13T18:10:45.221418Z",
          "iopub.status.idle": "2022-03-13T18:10:45.612759Z",
          "shell.execute_reply.started": "2022-03-13T18:10:45.221369Z",
          "shell.execute_reply": "2022-03-13T18:10:45.612002Z"
        },
        "trusted": true,
        "id": "Ni2NYmjnuwBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Create embedding matrix\"\"\"\n",
        "embedding_matrix = np.zeros((10000, embedding_dim))\n",
        "for idx, word in enumerate(tokenizer.get_vocabulary()):\n",
        "    try:\n",
        "        embedding_vector = loaded_model_100[word]\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "    except KeyError:\n",
        "        vec = np.zeros(embedding_dim)\n",
        "        embedding_matrix[idx] = vec"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:10:45.614215Z",
          "iopub.execute_input": "2022-03-13T18:10:45.614486Z",
          "iopub.status.idle": "2022-03-13T18:10:45.665624Z",
          "shell.execute_reply.started": "2022-03-13T18:10:45.614452Z",
          "shell.execute_reply": "2022-03-13T18:10:45.664953Z"
        },
        "trusted": true,
        "id": "doNcN0x0uwBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Νευρωνικό Μοντέλο\n",
        "\n",
        "Ο τρόπος λειτουργίας του νευρωνικού δικτύου βασίζεται στο [Show, Attend and Tell](https://arxiv.org/pdf/1502.03044.pdf). Συγκεκριμένα ακολουθεί τα παρακάτω βήματα: \n",
        "* τα χαρακτηριστικά εξάγονται από το τελευταίο συνελκτικό επίπεδο του InceptionV3 δίνοντας ένα διάνυσμα (8,8,2048)\n",
        "* τα διανύσματα αυτά αναδιατάσσονται σε διαστάσεις (64,2048)\n",
        "* τα διανύσματα αυτά μετά περνάνε στο CNN Encoder, ο οποίος αποτελείται ένα fully connected layer\n",
        "* ύστερα το RNN, σε αυτή την περίπτωση το GRU, σκανάρει αναδρομικά την εικόνα για προβλέψει την επόμενη λέξη"
      ],
      "metadata": {
        "id": "c_YEpjQsuwBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############\n",
        "\"\"\" Models \"\"\"\n",
        "##############\n",
        "\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "        # hidden shape == (batch_size, hidden_size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "        # attention_hidden_layer shape == (batch_size, 64, units)\n",
        "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        # score shape == (batch_size, 64, 1)\n",
        "        # This gives you an unnormalized score for each image feature.\n",
        "        score = self.V(attention_hidden_layer)\n",
        "\n",
        "        # attention_weights shape == (batch_size, 64, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "        # apply Dropout\n",
        "        self.dropout = tf.keras.layers.Dropout(0.15)\n",
        "        \n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x\n",
        "\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size, gru_OR_lstm):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.units = units\n",
        "        \n",
        "        # new Embedding layer\n",
        "        \n",
        "        self.embedding_layer = tf.keras.layers.Embedding(10000,\n",
        "                                                         embedding_dim,\n",
        "                                                         weights=[embedding_matrix],\n",
        "                                                         trainable=False)\n",
        "        \n",
        "        # Declaration of GRU\n",
        "        self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "        # Declaration of LSTM\n",
        "        self.lstm = tf.keras.layers.LSTM(self.units,\n",
        "                                        return_sequences=True,\n",
        "                                        return_state=True,\n",
        "                                        recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "        # boolean to decide which model\n",
        "        self.gru_OR_lstm = gru_OR_lstm\n",
        "        \n",
        "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        # apply Dropout\n",
        "        self.dropout = tf.keras.layers.Dropout(0.15)\n",
        "        \n",
        "        self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "    def call(self, x, features, hidden):\n",
        "        # defining attention as a separate model\n",
        "        context_vector, attention_weights = self.attention(features, hidden)\n",
        "        \n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        #x = self.embedding(x)\n",
        "        x = self.embedding_layer(x)\n",
        "        \n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        if(self.gru_OR_lstm==\"GRU\"):\n",
        "            # passing the concatenated vector to the GRU\n",
        "            output, state = self.gru(x)\n",
        "        \n",
        "        if(self.gru_OR_lstm==\"LSTM\"):\n",
        "            # passing the concatenated vector to the GRU\n",
        "            output, state, carry_state = self.lstm(x)\n",
        "            \n",
        "        # shape == (batch_size, max_length, hidden_size)\n",
        "        x = self.fc1(output)\n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # x shape == (batch_size * max_length, hidden_size)\n",
        "        x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size * max_length, vocab)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))\n",
        "        \n",
        "        "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:10:45.668066Z",
          "iopub.execute_input": "2022-03-13T18:10:45.668328Z",
          "iopub.status.idle": "2022-03-13T18:10:45.698000Z",
          "shell.execute_reply.started": "2022-03-13T18:10:45.668292Z",
          "shell.execute_reply": "2022-03-13T18:10:45.697344Z"
        },
        "trusted": true,
        "id": "VBlBIMf9uwBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, tokenizer.vocabulary_size(), \"GRU\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:10:45.699216Z",
          "iopub.execute_input": "2022-03-13T18:10:45.699626Z",
          "iopub.status.idle": "2022-03-13T18:10:45.744631Z",
          "shell.execute_reply.started": "2022-03-13T18:10:45.699590Z",
          "shell.execute_reply": "2022-03-13T18:10:45.743987Z"
        },
        "trusted": true,
        "id": "7R-CuF4cuwBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:10:45.745811Z",
          "iopub.execute_input": "2022-03-13T18:10:45.746027Z",
          "iopub.status.idle": "2022-03-13T18:10:45.752333Z",
          "shell.execute_reply.started": "2022-03-13T18:10:45.745996Z",
          "shell.execute_reply": "2022-03-13T18:10:45.751172Z"
        },
        "trusted": true,
        "id": "Q5gbqNazuwBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpoint"
      ],
      "metadata": {
        "id": "OWofWhGSuwBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################\n",
        "\"\"\" Checkpoint \"\"\"\n",
        "##################\n",
        "\n",
        "\n",
        "checkpoint_path = \"./checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,decoder=decoder,optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "    # restoring the latest checkpoint in checkpoint_path\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:10:45.753910Z",
          "iopub.execute_input": "2022-03-13T18:10:45.754202Z",
          "iopub.status.idle": "2022-03-13T18:10:45.769734Z",
          "shell.execute_reply.started": "2022-03-13T18:10:45.754146Z",
          "shell.execute_reply": "2022-03-13T18:10:45.769064Z"
        },
        "trusted": true,
        "id": "O_Rxy_C2uwBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "h-t45lgMuwBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################\n",
        "\"\"\" Training \"\"\"\n",
        "################\n",
        "\n",
        "# adding this in a separate cell because if you run the training cell\n",
        "# many times, the loss_plot array will be reset\n",
        "loss_plot = []\n",
        "\n",
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    \n",
        "    # initializing the hidden state for each batch\n",
        "    # because the captions are not related from image to image\n",
        "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "    \n",
        "    dec_input = tf.expand_dims((loaded_model_100.get_vector(\"#\")) * target.shape[0],1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        features = encoder(img_tensor)\n",
        "\n",
        "        for i in range(1, target.shape[1]):\n",
        "            # passing the features through the decoder\n",
        "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "            #print(\"predictions\",predictions)\n",
        "            loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "    total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    return loss, total_loss\n",
        "\n",
        "start_epoch = 0\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        ckpt_manager.save()\n",
        "\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
        "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n",
        "\n",
        "plt.plot(loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:10:45.771226Z",
          "iopub.execute_input": "2022-03-13T18:10:45.771707Z",
          "iopub.status.idle": "2022-03-13T18:20:58.375375Z",
          "shell.execute_reply.started": "2022-03-13T18:10:45.771662Z",
          "shell.execute_reply": "2022-03-13T18:20:58.373955Z"
        },
        "trusted": true,
        "id": "mhYPpmJYuwBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Captioning Evaluation\n",
        "\n",
        "Στα παρακάτω κελιά υλοποιείται η αξιολόγηση των captions που προβλέπει το δίκτυο με δίκη μας κρίση, δηλαδή με τις παρακάτω συναρτήσεις προκύπτουν στην έξοδο το αληθινό caption και το προβλεπόμενο από το δίκτυο και με δική μας κρίση αποφασίζουμε αν ήταν επιτυχές το caption για την αντίστοιχη εικόνα. "
      ],
      "metadata": {
        "id": "V5_NSm5cuwBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Δοκιμάσαμε στο evaluation να χρησιμοποιήσουμε beam_search για τα prediction_captions . O κώδικας που χρησιμοποιήσαμε είναι ο ακόλουθος:\n",
        "\n",
        "def beam_evaluation(image):\n",
        "    \n",
        "    max_length=20\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "    beam_index=3\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
        "                                                 -1,\n",
        "                                                 img_tensor_val.shape[3]))\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([word_to_index(tf.constant('<start>'))], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input,\n",
        "                                                         features,\n",
        "                                                         hidden)\n",
        "        prob=[]\n",
        "        zeygos=[]\n",
        "        predictions=predictions.numpy()\n",
        "        predictions=predictions[0]\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "        \n",
        "        Beam_search\n",
        "        predicted_id = tf.argsort(predictions, axis=1)[0].numpy()\n",
        "        predicted_id=predicted_id[:beam_index]\n",
        "        \n",
        "        for pred in predicted_id:\n",
        "            predicted_word =\n",
        "                      tf.compat.as_text(index_to_word(tf.constant(pred)).numpy())\n",
        "            result.append(predicted_word)\n",
        "            \n",
        "            probabilliti=predictions[pred]\n",
        "            prob.append(probabilities)\n",
        "            probabilliti+=probability\n",
        "            zeygos.append((predicted_word,probabilliti))\n",
        "           if predicted_word == '<end>':\n",
        "               return result, attention_plot\n",
        "\n",
        "           dec_input = tf.expand_dims([predicted_id], 0)\n",
        "        result=np.argmax(result)\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-Z4AUmo_uwBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Παρόλα αυτά παρατηρήσαμε στα captions ότι ενώ οι αρχικές λέξεις που προέβλεπε ήταν καλές στην συνέχεια 'χανόταν' με αποτέλεσμα το bleu score να είναι πολύ χαμηλό.(Προφανώς η υλοποίηση μας κάπου μπάζει). Αντ'αυτού χρησιμοποιήσαμε Greedy Search."
      ],
      "metadata": {
        "id": "VzaYYzOTuwBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(image):\n",
        "    max_length=20\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
        "                                                 -1,\n",
        "                                                 img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([word_to_index(tf.constant('<start>'))], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input,\n",
        "                                                         features,\n",
        "                                                         hidden)\n",
        "        \n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "        \n",
        "        #Greedy_search\n",
        "        predicted_id = tf.math.argmax(predictions, axis=1)[0].numpy()\n",
        "        \n",
        "        predicted_word = tf.compat.as_text(index_to_word(tf.constant(predicted_id)).numpy())\n",
        "        result.append(predicted_word)\n",
        "\n",
        "        if predicted_word == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot\n",
        "\n",
        "def plot_attention(image, result, attention_plot):\n",
        "    temp_image = np.array(Image.open(image))\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for i in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[i], (8, 8))\n",
        "        grid_size = max(int(np.ceil(len_result/2)), 2)\n",
        "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
        "        ax.set_title(result[i])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:20:58.382549Z",
          "iopub.execute_input": "2022-03-13T18:20:58.382749Z",
          "iopub.status.idle": "2022-03-13T18:20:58.395210Z",
          "shell.execute_reply.started": "2022-03-13T18:20:58.382724Z",
          "shell.execute_reply": "2022-03-13T18:20:58.394289Z"
        },
        "trusted": true,
        "id": "cqvBVXkXuwBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Real_Pred_Caption_Printer(rid):\n",
        "    # captions on the validation set\n",
        "    image = img_name_val[rid]\n",
        "    real_caption =image_path_to_train_caption[image]\n",
        "    #real_caption = ' '.join([tf.compat.as_text(index_to_word(tf.constant(i)).numpy()) for i in cap_val[rid] if i not in [0]])\n",
        "    result, attention_plot = evaluate(image)\n",
        "    print('len',len(result))\n",
        "\n",
        "    print('Real Caption:', real_caption)\n",
        "    print('Prediction Caption:', ' '.join(result))\n",
        "    plot_attention(image, result, attention_plot)\n",
        "    \n",
        "    return real_caption, result"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:20:58.399222Z",
          "iopub.execute_input": "2022-03-13T18:20:58.399669Z",
          "iopub.status.idle": "2022-03-13T18:20:58.430337Z",
          "shell.execute_reply.started": "2022-03-13T18:20:58.399637Z",
          "shell.execute_reply": "2022-03-13T18:20:58.429517Z"
        },
        "trusted": true,
        "id": "PEqsKz1xuwBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### random image captioning ###\n",
        "RID = np.random.randint(0, len(img_name_val))\n",
        "print(\"The id of the image is: \",img_name_val[RID])\n",
        "_,_ = Real_Pred_Caption_Printer(RID)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:20:58.432715Z",
          "iopub.execute_input": "2022-03-13T18:20:58.433029Z",
          "iopub.status.idle": "2022-03-13T18:21:01.302168Z",
          "shell.execute_reply.started": "2022-03-13T18:20:58.432993Z",
          "shell.execute_reply": "2022-03-13T18:21:01.301478Z"
        },
        "trusted": true,
        "id": "ZOCk3Fz7uwBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cherry Picking \n",
        "Στο επόμενο κελί δίνονται κάποια ids καλών παραδειγμάτων image captioning από το μοντέλο, που δημιουργήσαμε μέχρι στιγμής."
      ],
      "metadata": {
        "id": "_QOr3Z_HuwBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(img_name_val)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:21:01.303455Z",
          "iopub.execute_input": "2022-03-13T18:21:01.303856Z",
          "iopub.status.idle": "2022-03-13T18:21:01.309418Z",
          "shell.execute_reply.started": "2022-03-13T18:21:01.303818Z",
          "shell.execute_reply": "2022-03-13T18:21:01.308753Z"
        },
        "trusted": true,
        "id": "PZon_xlGuwB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cherry_list = [100, 506, 1218, 2000, 1254]\n",
        "\n",
        "cherry_ref = []\n",
        "cherry_hypo = []\n",
        "for cherry in cherry_list:\n",
        "    cherry_ref_helper,cherry_hypo_helper = Real_Pred_Caption_Printer(cherry)\n",
        "    #cherry_ref.append(cherry_ref_helper)\n",
        "   # cherry_hypo.append(cherry_hypo_helper)\n",
        "    #axristi_diaxorisi()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:21:01.310811Z",
          "iopub.execute_input": "2022-03-13T18:21:01.311239Z",
          "iopub.status.idle": "2022-03-13T18:21:11.193964Z",
          "shell.execute_reply.started": "2022-03-13T18:21:01.311204Z",
          "shell.execute_reply": "2022-03-13T18:21:11.193250Z"
        },
        "trusted": true,
        "id": "vkAFDksFuwB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nir Picking\n",
        "\n",
        "Στο επόμενο κελί δίνονται κάποια ids καλών παραδειγμάτων image captioning από το μοντέλο, που δημιουργήσαμε μέχρι στιγμής."
      ],
      "metadata": {
        "id": "JX8hhsn9uwB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nir_list = [3005, 368, 2456, 1308, 1725]\n",
        "\n",
        "nir_ref = []\n",
        "nir_hypo = []\n",
        "for nir in nir_list:\n",
        "    nir_ref_helper,nir_hypo_helper = Real_Pred_Caption_Printer(nir)\n",
        "    nir_ref.append(nir_ref_helper)\n",
        "    nir_hypo.append(nir_hypo_helper)\n",
        "    axristi_diaxorisi()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:21:11.195221Z",
          "iopub.execute_input": "2022-03-13T18:21:11.195999Z",
          "iopub.status.idle": "2022-03-13T18:21:19.873821Z",
          "shell.execute_reply.started": "2022-03-13T18:21:11.195961Z",
          "shell.execute_reply": "2022-03-13T18:21:19.873076Z"
        },
        "trusted": true,
        "id": "x5Mc5QF9uwB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation set's references & hypothesis"
      ],
      "metadata": {
        "id": "9NMcgGkTuwB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################\n",
        "\"\"\" Validation Set passed through Real_Pred_Caption_Printer \"\"\"\n",
        "###############################################################\n",
        "\n",
        "\n",
        "val_ref = []\n",
        "val_hypo = []\n",
        "for inv_id,inv in enumerate(img_name_val):\n",
        "    val_hypo_helper = evaluate(inv)[0]\n",
        "    val_hypo.append(val_hypo_helper)\n",
        "    val_ref_helper = ' '.join([tf.compat.as_text(index_to_word(tf.constant(i)).numpy()) for i in cap_val[inv_id] if i not in [0]])\n",
        "    val_ref.append(val_ref_helper)\n",
        "    \n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T18:21:19.875444Z",
          "iopub.execute_input": "2022-03-13T18:21:19.875970Z"
        },
        "trusted": true,
        "id": "KtePTMgduwB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Αξιολόγηση της ποιότητας του captioning\n",
        "\n",
        "Για την εκτίμηση της ποιότητας του παραγόμενου captioning θα χρησιμοποιηθεί το BLEU (Bilingual Evaluation Understudy) score, μεταξύ hypothesis(το caption του νευρωνικού) και references(το αληθινό caption). Το BLEU είναι μια μετρική για αξιολογήσης μοντέλων παραγωγής (και μετάφρασης) κειμένων. \n",
        "\n",
        "Συγκεκριμένα, είναι ένας αριθμός μεταξύ του 0 και του 1, ο οποίος αντιπροσωπεύει την ομοιότητα των παραγόμενων κειμένων από τα διάφορα μοντέλα σε σύγκριση με κάποια καλά captions (ή μεταφράσεις αντιστοίχως). Τιμή BLEU ίση με το 0 σημαίνει ότι δεν υπήρχε καμία επικάλυψη μεταξύ των λέξεων προβλεπόμενου και πραγματικού κειμένου. Αντιθέτως, τιμή BLEU ίση με τη 1 σημαίνει ότι υπήρχε πλήρης επικάλυψη μεταξύ των λέξεων προβλεπόμενου και πραγματικού κειμένου. Έχει αποδειχθεί ότι το BLEU score σχετίζεται σημαντικά με την ανθρώπινη εκτίμηση για την επιτύχια captioning (και μετάφρασης κειμένου).\n",
        "\n",
        "Για τον υπολογισμού των BLEU scores η NLTK στο [nltk.translate.bleu_score](https://www.nltk.org/_modules/nltk/translate/bleu_score.html) παρέχει τις απαραίτητες συναρτήσεις : \n",
        "* την _sentence_bleu_ , για την αξιολόγηση του captioning ενός μεμονωμένου παραδείγματος\n",
        "* την _corpus_bleu_ , για την αξιολόγηση του captioning ολόκληρου set εικόνων\n",
        "\n",
        "Σε όλες τις περιπτώσεις χρησιμοποιούνται οι εξής παραμέτροι: _weights=(0.4, 0.3, 0.2, 0.1)_ και _smoothing_function=SmoothingFunction().method1_"
      ],
      "metadata": {
        "id": "DwxJbFq-uwB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "\"\"\" BLEU score implementation \"\"\"\n",
        "#################################\n",
        "\n",
        "import math\n",
        "import sys\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from fractions import Fraction\n",
        "from nltk.util import ngrams\n",
        "\n",
        "\n",
        "def sentence_bleu(references,hypothesis,weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=None,auto_reweigh=False):\n",
        "    \n",
        "    \"\"\" PARAMETER ANALYSIS\n",
        "        * param references: reference sentences\n",
        "        * type references: list(list(str))\n",
        "        * param hypothesis: a hypothesis sentence\n",
        "        * type hypothesis: list(str)\n",
        "        * param weights: weights for unigrams, bigrams, trigrams and so on (one or a list of weights)\n",
        "        * type weights: tuple(float) / list(tuple(float))\n",
        "        * param smoothing_function:\n",
        "        * type smoothing_function: SmoothingFunction\n",
        "        * param auto_reweigh: Option to re-normalize the weights uniformly.\n",
        "        * type auto_reweigh: bool\n",
        "        * return: The sentence-level BLEU score. Returns a list if multiple weights were supplied.\n",
        "        * rtype: float / list(float\n",
        "    \"\"\"\n",
        "    \n",
        "    The_Return = corpus_bleu([references], [hypothesis], weights, smoothing_function, auto_reweigh)\n",
        "    \n",
        "    return The_Return\n",
        "\n",
        "def corpus_bleu(list_of_references,hypotheses,weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=None,auto_reweigh=False):\n",
        "    \n",
        "    \"\"\" PARAMETER ANALYSIS\n",
        "        * param list_of_references: a corpus of lists of reference sentences, w.r.t. hypotheses\n",
        "        * type list_of_references: list(list(list(str)))\n",
        "        * param hypotheses: a list of hypothesis sentences\n",
        "        * type hypotheses: list(list(str))\n",
        "        * param weights: weights for unigrams, bigrams, trigrams and so on (one or a list of weights)\n",
        "        * type weights: tuple(float) / list(tuple(float))\n",
        "        * param smoothing_function:\n",
        "        * type smoothing_function: SmoothingFunction\n",
        "        * param auto_reweigh: Option to re-normalize the weights uniformly.\n",
        "        * type auto_reweigh: bool\n",
        "        * return: The corpus-level BLEU score.\n",
        "        * rtype: float\n",
        "    \"\"\"\n",
        "    # Before proceeding to compute BLEU, perform sanity checks.\n",
        "\n",
        "    p_numerators = Counter()  # Key = ngram order, and value = no. of ngram matches.\n",
        "    p_denominators = Counter()  # Key = ngram order, and value = no. of ngram in ref.\n",
        "    hyp_lengths, ref_lengths = 0, 0\n",
        "\n",
        "    assert len(list_of_references) == len(hypotheses), (\n",
        "        \"The number of hypotheses and their reference(s) should be the \" \"same \"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        weights[0][0]\n",
        "    except TypeError:\n",
        "        weights = [weights]\n",
        "    max_weight_length = max(len(weight) for weight in weights)\n",
        "\n",
        "    # Iterate through each hypothesis and their corresponding references.\n",
        "    \n",
        "    for references, hypothesis in zip(list_of_references, hypotheses):\n",
        "        \n",
        "        # For each order of ngram, calculate the numerator and\n",
        "        # denominator for the corpus-level modified precision.\n",
        "        \n",
        "        for i in range(1, max_weight_length + 1):\n",
        "            p_i = modified_precision(references, hypothesis, i)\n",
        "            p_numerators[i] += p_i.numerator\n",
        "            p_denominators[i] += p_i.denominator\n",
        "\n",
        "        # Calculate the hypothesis length and the closest reference length.\n",
        "        # Adds them to the corpus-level hypothesis and reference counts.\n",
        "        \n",
        "        hyp_len = len(hypothesis)\n",
        "        hyp_lengths += hyp_len\n",
        "        ref_lengths += closest_ref_length(references, hyp_len)\n",
        "\n",
        "    # Calculate corpus-level brevity penalty.\n",
        "    bp = brevity_penalty(ref_lengths, hyp_lengths)\n",
        "\n",
        "    # Collects the various precision values for the different ngram orders.\n",
        "    p_n = [\n",
        "        Fraction(p_numerators[i], p_denominators[i], _normalize=False)\n",
        "        for i in range(1, max_weight_length + 1)\n",
        "    ]\n",
        "\n",
        "    # Returns 0 if there's no matching n-grams\n",
        "    # We only need to check for p_numerators[1] == 0, since if there's\n",
        "    # no unigrams, there won't be any higher order ngrams.\n",
        "    if p_numerators[1] == 0:\n",
        "        return 0 if len(weights) == 1 else [0] * len(weights)\n",
        "\n",
        "    # If there's no smoothing, set use method0 from SmoothinFunction class.\n",
        "    if not smoothing_function:\n",
        "        smoothing_function = SmoothingFunction().method0\n",
        "    # Smoothen the modified precision.\n",
        "    # Note: smoothing_function() may convert values into floats;\n",
        "    #       it tries to retain the Fraction object as much as the\n",
        "    #       smoothing method allows.\n",
        "    p_n = smoothing_function(\n",
        "        p_n, references=references, hypothesis=hypothesis, hyp_len=hyp_lengths\n",
        "    )\n",
        "\n",
        "    bleu_scores = []\n",
        "    for weight in weights:\n",
        "        # Uniformly re-weighting based on maximum hypothesis lengths if largest\n",
        "        # order of n-grams < 4 and weights is set at default.\n",
        "        if auto_reweigh:\n",
        "            if hyp_lengths < 4 and weight == (0.25, 0.25, 0.25, 0.25):\n",
        "                weight = (1 / hyp_lengths,) * hyp_lengths\n",
        "\n",
        "        s = (w_i * math.log(p_i) for w_i, p_i in zip(weight, p_n) if p_i > 0)\n",
        "        s = bp * math.exp(math.fsum(s))\n",
        "        bleu_scores.append(s)\n",
        "    \n",
        "    return bleu_scores[0] if len(weights) == 1 else bleu_scores\n",
        "\n",
        "\n",
        "def modified_precision(references, hypothesis, n):\n",
        "    \n",
        "    \"\"\" PARAMETER ANALYSIS\n",
        "        * param references: A list of reference translations.\n",
        "        * type references: list(list(str))\n",
        "        * param hypothesis: A hypothesis translation.\n",
        "        * type hypothesis: list(str)\n",
        "        * param n: The ngram order.\n",
        "        * type n: int\n",
        "        * return: BLEU's modified precision for the nth order ngram.\n",
        "        * rtype: Fraction\n",
        "    \"\"\"\n",
        "    \n",
        "    # Extracts all ngrams in hypothesis\n",
        "    # Set an empty Counter if hypothesis is empty.\n",
        "    counts = Counter(ngrams(hypothesis, n)) if len(hypothesis) >= n else Counter()\n",
        "    \n",
        "    # Extract a union of references' counts.\n",
        "    # max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])\n",
        "    max_counts = {}\n",
        "    for reference in references:\n",
        "        reference_counts = (\n",
        "            Counter(ngrams(reference, n)) if len(reference) >= n else Counter()\n",
        "        )\n",
        "        for ngram in counts:\n",
        "            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n",
        "\n",
        "    # Assigns the intersection between hypothesis and references' counts.\n",
        "    clipped_counts = {\n",
        "        ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()\n",
        "    }\n",
        "\n",
        "    numerator = sum(clipped_counts.values())\n",
        "    \n",
        "    # Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\n",
        "    # Usually this happens when the ngram order is > len(reference).\n",
        "    denominator = max(1, sum(counts.values()))\n",
        "\n",
        "    return Fraction(numerator, denominator, _normalize=False)\n",
        "    \n",
        "def closest_ref_length(references, hyp_len):\n",
        "    \n",
        "    \"\"\" PARAMETER ANALYSIS\n",
        "    This function finds the reference that is the closest length to the\n",
        "    hypothesis. The closest reference length is referred to as *r* variable\n",
        "    from the brevity penalty formula in Papineni et. al. (2002)\n",
        "\n",
        "        * param references: A list of reference translations.\n",
        "        * type references: list(list(str))\n",
        "        * param hyp_len: The length of the hypothesis.\n",
        "        * type hyp_len: int\n",
        "        * return: The length of the reference that's closest to the hypothesis.\n",
        "        * rtype: int\n",
        "    \"\"\"\n",
        "    ref_lens = (len(reference) for reference in references)\n",
        "    closest_ref_len = min(\n",
        "        ref_lens, key=lambda ref_len: (abs(ref_len - hyp_len), ref_len)\n",
        "    )\n",
        "    return closest_ref_len\n",
        "\n",
        "def brevity_penalty(closest_ref_len, hyp_len):\n",
        "    \"\"\" PARAMETER ANALYSIS\n",
        "        * param hyp_len: The length of the hypothesis for a single sentence OR the\n",
        "            sum of all the hypotheses' lengths for a corpus\n",
        "        * type hyp_len: int\n",
        "        * param closest_ref_len: The length of the closest reference for a single\n",
        "            hypothesis OR the sum of all the closest references for every hypotheses.\n",
        "        * type closest_ref_len: int\n",
        "        * return: BLEU's brevity penalty.\n",
        "        * rtype: float\n",
        "    \"\"\"\n",
        "    \n",
        "    if hyp_len > closest_ref_len:\n",
        "        return 1\n",
        "    # If hypothesis is empty, brevity penalty = 0 should result in BLEU = 0.0\n",
        "    elif hyp_len == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return math.exp(1 - closest_ref_len / hyp_len)\n",
        "\n",
        "class SmoothingFunction:\n",
        "    \"\"\"\n",
        "    This is an implementation of the smoothing techniques\n",
        "    for segment-level BLEU scores that was presented in\n",
        "    Boxing Chen and Collin Cherry (2014) A Systematic Comparison of\n",
        "    Smoothing Techniques for Sentence-Level BLEU. In WMT14.\n",
        "    http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, epsilon=0.1, alpha=5, k=5):\n",
        "        \"\"\" PARAMETER ANALYSIS\n",
        "            * param epsilon: the epsilon value use in method 1\n",
        "            * type epsilon: float\n",
        "            * param alpha: the alpha value use in method 6\n",
        "            * type alpha: int\n",
        "            * param k: the k value use in method 4\n",
        "            * type k: int\n",
        "        \"\"\"\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.k = k\n",
        "    \n",
        "    def method0(self, p_n, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        No smoothing.\n",
        "        \"\"\"\n",
        "        p_n_new = []\n",
        "        for i, p_i in enumerate(p_n):\n",
        "            if p_i.numerator != 0:\n",
        "                p_n_new.append(p_i)\n",
        "            else:\n",
        "                _msg = str(\n",
        "                    \"\\nThe hypothesis contains 0 counts of {}-gram overlaps.\\n\"\n",
        "                    \"Therefore the BLEU score evaluates to 0, independently of\\n\"\n",
        "                    \"how many N-gram overlaps of lower order it contains.\\n\"\n",
        "                    \"Consider using lower n-gram order or use \"\n",
        "                    \"SmoothingFunction()\"\n",
        "                ).format(i + 1)\n",
        "                warnings.warn(_msg)\n",
        "                # When numerator==0 where denonminator==0 or !=0, the result\n",
        "                # for the precision score should be equal to 0 or undefined.\n",
        "                # Due to BLEU geometric mean computation in logarithm space,\n",
        "                # we we need to take the return sys.float_info.min such that\n",
        "                # math.log(sys.float_info.min) returns a 0 precision score.\n",
        "                p_n_new.append(sys.float_info.min)\n",
        "        \n",
        "        return p_n_new\n",
        "    \n",
        "    def method1(self, p_n, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Smoothing method 1: Add *epsilon* counts to precision with 0 counts.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            (p_i.numerator + self.epsilon) / p_i.denominator\n",
        "            if p_i.numerator == 0\n",
        "            else p_i\n",
        "            for p_i in p_n\n",
        "        ]\n",
        "    \n",
        "    def method2(self, p_n, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Smoothing method 2: Add 1 to both numerator and denominator from\n",
        "        Chin-Yew Lin and Franz Josef Och (2004) ORANGE: a Method for\n",
        "        Evaluating Automatic Evaluation Metrics for Machine Translation.\n",
        "        In COLING 2004.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            Fraction(p_n[i].numerator + 1, p_n[i].denominator + 1, _normalize=False)\n",
        "            if i != 0\n",
        "            else p_n[0]\n",
        "            for i in range(len(p_n))\n",
        "        ]\n",
        "    \n",
        "    def method3(self, p_n, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Smoothing method 3: NIST geometric sequence smoothing\n",
        "        The smoothing is computed by taking 1 / ( 2^k ), instead of 0, for each\n",
        "        precision score whose matching n-gram count is null.\n",
        "        k is 1 for the first 'n' value for which the n-gram match count is null/\n",
        "\n",
        "        For example, if the text contains:\n",
        "\n",
        "        - one 2-gram match\n",
        "        - and (consequently) two 1-gram matches\n",
        "\n",
        "        the n-gram count for each individual precision score would be:\n",
        "\n",
        "        - n=1  =>  prec_count = 2     (two unigrams)\n",
        "        - n=2  =>  prec_count = 1     (one bigram)\n",
        "        - n=3  =>  prec_count = 1/2   (no trigram,  taking 'smoothed' value of 1 / ( 2^k ), with k=1)\n",
        "        - n=4  =>  prec_count = 1/4   (no fourgram, taking 'smoothed' value of 1 / ( 2^k ), with k=2)\n",
        "        \"\"\"\n",
        "        incvnt = 1  # From the mteval-v13a.pl, it's referred to as k.\n",
        "        for i, p_i in enumerate(p_n):\n",
        "            if p_i.numerator == 0:\n",
        "                p_n[i] = 1 / (2 ** incvnt * p_i.denominator)\n",
        "                incvnt += 1\n",
        "        return p_n\n",
        "    \n",
        "    def method4(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Smoothing method 4:\n",
        "        Shorter translations may have inflated precision values due to having\n",
        "        smaller denominators; therefore, we give them proportionally\n",
        "        smaller smoothed counts. Instead of scaling to 1/(2^k), Chen and Cherry\n",
        "        suggests dividing by 1/ln(len(T)), where T is the length of the translation.\n",
        "        \"\"\"\n",
        "        incvnt = 1\n",
        "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
        "        for i, p_i in enumerate(p_n):\n",
        "            if p_i.numerator == 0 and hyp_len > 1:\n",
        "                # incvnt = i + 1 * self.k / math.log(\n",
        "                #     hyp_len\n",
        "                # )  # Note that this K is different from the K from NIST.\n",
        "                # p_n[i] = incvnt / p_i.denominator\\\n",
        "                numerator = 1 / (2 ** incvnt * self.k / math.log(hyp_len))\n",
        "                p_n[i] = numerator / p_i.denominator\n",
        "                incvnt += 1\n",
        "        return p_n\n",
        "    \n",
        "    def method5(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Smoothing method 5:\n",
        "        The matched counts for similar values of n should be similar. To a\n",
        "        calculate the n-gram matched count, it averages the n−1, n and n+1 gram\n",
        "        matched counts.\n",
        "        \"\"\"\n",
        "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
        "        m = {}\n",
        "        # Requires an precision value for an addition ngram order.\n",
        "        p_n_plus1 = p_n + [modified_precision(references, hypothesis, 5)]\n",
        "        m[-1] = p_n[0] + 1\n",
        "        for i, p_i in enumerate(p_n):\n",
        "            p_n[i] = (m[i - 1] + p_i + p_n_plus1[i + 1]) / 3\n",
        "            m[i] = p_n[i]\n",
        "        return p_n\n",
        "    \n",
        "    def method6(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Smoothing method 6:\n",
        "        Interpolates the maximum likelihood estimate of the precision *p_n* with\n",
        "        a prior estimate *pi0*. The prior is estimated by assuming that the ratio\n",
        "        between pn and pn−1 will be the same as that between pn−1 and pn−2; from\n",
        "        Gao and He (2013) Training MRF-Based Phrase Translation Models using\n",
        "        Gradient Ascent. In NAACL.\n",
        "        \"\"\"\n",
        "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
        "        # This smoothing only works when p_1 and p_2 is non-zero.\n",
        "        # Raise an error with an appropriate message when the input is too short\n",
        "        # to use this smoothing technique.\n",
        "        assert p_n[2], \"This smoothing method requires non-zero precision for bigrams.\"\n",
        "        for i, p_i in enumerate(p_n):\n",
        "            if i in [0, 1]:  # Skips the first 2 orders of ngrams.\n",
        "                continue\n",
        "            else:\n",
        "                pi0 = 0 if p_n[i - 2] == 0 else p_n[i - 1] ** 2 / p_n[i - 2]\n",
        "                # No. of ngrams in translation that matches the reference.\n",
        "                m = p_i.numerator\n",
        "                # No. of ngrams in translation.\n",
        "                l = sum(1 for _ in ngrams(hypothesis, i + 1))\n",
        "                # Calculates the interpolated precision.\n",
        "                p_n[i] = (m + self.alpha * pi0) / (l + self.alpha)\n",
        "        return p_n\n",
        "    \n",
        "    def method7(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Smoothing method 7:\n",
        "        Interpolates methods 4 and 5.\n",
        "        \"\"\"\n",
        "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
        "        p_n = self.method4(p_n, references, hypothesis, hyp_len)\n",
        "        p_n = self.method5(p_n, references, hypothesis, hyp_len)\n",
        "        return p_n"
      ],
      "metadata": {
        "trusted": true,
        "id": "x5zSkvVbuwB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#declare the BLEU standard parameters\n",
        "Weights = [0.4, 0.3, 0.2, 0.1]\n",
        "SmoothFunc = SmoothingFunction().method1\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "RBwLC2vzuwB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To BLEU score για μια τυχαία εικόνα της επιλογής μας"
      ],
      "metadata": {
        "id": "MU9Fw5b3uwB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reals_1254, hypothesis_1254 = Real_Pred_Caption_Printer(1254)\n",
        "bleu_1254 = sentence_bleu(reals_1254, hypothesis_1254,weights=Weights,smoothing_function=SmoothFunc,auto_reweigh=False)\n",
        "print(\"The bleu score for 1254 is :\"+str(bleu_1254))"
      ],
      "metadata": {
        "trusted": true,
        "id": "F06OMHQXuwB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To BLEU score για τα cherries, που επιλέχθηκαν."
      ],
      "metadata": {
        "id": "QYnI3rh0uwB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_cherries = corpus_bleu(cherry_ref,cherry_hypo,weights=Weights,smoothing_function=SmoothFunc,auto_reweigh=False)\n",
        "print(\"The bleu score for the cherries picked are :\",bleu_cherries)"
      ],
      "metadata": {
        "trusted": true,
        "id": "CtLF9NPbuwB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To BLEU score για τα nir, που επιλέχθηκαν."
      ],
      "metadata": {
        "id": "Rccc96PDuwB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_nir = corpus_bleu(nir_ref,nir_hypo,weights=Weights,smoothing_function=SmoothFunc,auto_reweigh=False)\n",
        "print(\"The bleu score for the nir picked are :\",bleu_nir)"
      ],
      "metadata": {
        "trusted": true,
        "id": "t7bbeMM8uwB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To BLEU score για τo Validation set"
      ],
      "metadata": {
        "id": "PvcHLrK8uwB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_val = corpus_bleu(val_ref,val_hypo,weights=Weights,smoothing_function=SmoothFunc,auto_reweigh=False)\n",
        "print(\"The bleu score for the validation set are :\",bleu_val)"
      ],
      "metadata": {
        "trusted": true,
        "id": "nRUZs9uKuwB3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}